{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Drone Classification Model (Resnet)\n",
    "#### John D. Valencia & Max Blumenfeld - Senior Projects 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries\n",
    "\n",
    "First, we'll import all the necessary libraries required for data handling, model building, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow warnings for cleaner output\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# Set logging level to ERROR to minimize warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Verify TensorFlow is using the GPU (Apple Silicon)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "print(os.getcwd())\n",
    "\n",
    "DATA_DIR = 'hybrid_data_split'  # Update if different\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "MODEL_NAME = 'ResNet50'\n",
    "CLASS_NAMES = ['not_drone', 'drone']  # Ensure this matches your directory structure\n",
    "\n",
    "# Verify data directory exists\n",
    "assert os.path.isdir(DATA_DIR), f\"Data directory {DATA_DIR} does not exist.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Data\n",
    "\n",
    "Load the training, validation, and test datasets using image_dataset_from_directory. We'll also apply caching, shuffling, and prefetching for optimized data pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_dir, image_size=(224, 224), batch_size=32):\n",
    "    \"\"\"\n",
    "    Loads training, validation, and test datasets from the specified directory.\n",
    "    \"\"\"\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    print(\"Loading training dataset from:\", train_dir)\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=123\n",
    "    )\n",
    "    \n",
    "    print(\"Loading validation dataset from:\", val_dir)\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=123\n",
    "    )\n",
    "    \n",
    "    print(\"Loading test dataset from:\", test_dir)\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def configure_datasets(train_ds, val_ds, test_ds, buffer_size=1000, AUTOTUNE=tf.data.AUTOTUNE):\n",
    "    \"\"\"\n",
    "    Configures datasets for performance.\n",
    "    \"\"\"\n",
    "    train_ds = train_ds.cache().shuffle(buffer_size).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Load the datasets\n",
    "train_ds, val_ds, test_ds = load_datasets(DATA_DIR, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "# **Retrieve class names before configuring the datasets**\n",
    "class_names = train_ds.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Configure the datasets for performance\n",
    "train_ds, val_ds, test_ds = configure_datasets(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ResNet50 Model\n",
    "\n",
    "Construct the ResNet50-based model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_model(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Builds and compiles a ResNet50-based model for binary classification.\n",
    "    \"\"\"\n",
    "    # Load the ResNet50 model without the top classification layer\n",
    "    base_model = keras.applications.ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model to prevent its weights from being updated during initial training\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "    ])\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # Preprocessing for ResNet50\n",
    "    x = keras.applications.resnet50.preprocess_input(x)\n",
    "    \n",
    "    # Pass through the base model\n",
    "    x = base_model(x, training=False)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    x = layers.Dropout(0.4  )(x)\n",
    "    \n",
    "    # Output layer for binary classification\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "   \n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the ResNet50 model\n",
    "model = build_resnet_model(input_shape=IMAGE_SIZE + (3,))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='../models/resnet50_best_model.keras', \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Train the ResNet50 model using the training dataset and validate on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_history(history, model_name, dataset_type):\n",
    "#     \"\"\"\n",
    "#     Plots training and validation accuracy and loss.\n",
    "#     \"\"\"\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "    \n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "    \n",
    "#     epochs_range = range(len(acc))\n",
    "    \n",
    "#     plt.figure(figsize=(14, 6))\n",
    "    \n",
    "#     # Accuracy Plot\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "#     plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.title(f'{model_name} - {dataset_type} - Training and Validation Accuracy')\n",
    "    \n",
    "#     # Loss Plot\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs_range, loss, label='Training Loss')\n",
    "#     plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.title(f'{model_name} - {dataset_type} - Training and Validation Loss')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# # Plot training history\n",
    "# plot_training_history(history, MODEL_NAME, 'Real_Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_ds, class_names):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints classification metrics.\n",
    "    \"\"\"\n",
    "    # Get predictions and true labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        preds = model.predict(images)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend((preds > 0.5).astype(int).flatten())\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Normalize the confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]} ({cm_normalized[i, j]:.2f})\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "# evaluate_model(model, test_ds, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_trial():\n",
    "    \"\"\"Runs a single training trial and returns the metrics\"\"\"\n",
    "    \n",
    "    # Clear session and memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Load fresh datasets\n",
    "    train_ds, val_ds, test_ds = load_datasets(DATA_DIR, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
    "    train_ds, val_ds, test_ds = configure_datasets(train_ds, val_ds, test_ds)\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_resnet_model(input_shape=IMAGE_SIZE + (3,))\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Create fresh test dataset for evaluation\n",
    "    fresh_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        os.path.join(DATA_DIR, 'test'),\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        shuffle=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "    \n",
    "    # Evaluate using fresh dataset\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for images, labels in fresh_test_ds:\n",
    "        preds = model.predict(images, verbose=0)  # Set verbose=0 to reduce output\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend((preds > 0.5).astype(int).flatten())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single_trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in TensorFlow's SavedModel format\n",
    "model.save('../models/resnet50_model_2.keras')\n",
    "print(\"Model training and evaluation completed. Model saved to 'models/resnet50_model_2'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4970env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
